import os
from typing import List, Optional, Tuple

# Import for dotenv support
from dotenv import load_dotenv

from openai import OpenAI

from ..core import Rule, State
from ..utils import format_question
from .base_generator import QuestionGenerator

# Load environment variables from .env file
load_dotenv()


class LLMQuestionGenerator:
    """
    Uses an LLM to generate full questions based on trajectories created by other generators.
    Utilizes OpenRouter's API to access various LLM models.
    """

    def __init__(self, base_generator: QuestionGenerator, api_key: Optional[str] = None, site_url: Optional[str] = None, site_name: Optional[str] = None, model: Optional[str] = None):
        """
        Initialize the LLM Question Generator.

        Args:
            base_generator: The base generator to use for creating trajectories
            api_key: OpenRouter API key (defaults to environment variable OPENROUTER_API_KEY)
            site_url: Your site URL for rankings on openrouter.ai (defaults to SITE_URL env var)
            site_name: Your site name for rankings on openrouter.ai (defaults to SITE_NAME env var)
            model: The LLM model to use (defaults to LLM_MODEL env var or "openai/gpt-4o")
        """
        self.base_generator = base_generator
        self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OpenRouter API key is required. Provide it as parameter or set OPENROUTER_API_KEY environment variable.")
        
        # Use environment variables as fallbacks for optional parameters
        self.site_url = site_url or os.getenv("SITE_URL")
        self.site_name = site_name or os.getenv("SITE_NAME")
        self.model = model or os.getenv("LLM_MODEL", "openai/gpt-4o")
        
        # Initialize OpenAI client with OpenRouter base URL
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.api_key,
        )

    def generate_from_trajectory(self, seed_state: State, applied_rules: List[Rule], configuration: List[State]) -> str:
        """
        Takes an existing trajectory and uses an LLM to generate a full, natural language question.

        Args:
            seed_state: The starting state of the trajectory
            applied_rules: The list of rules applied in the trajectory
            configuration: The list of states in the trajectory

        Returns:
            A natural language question generated by the LLM
        """
        # Format the trajectory into a structured question using the existing formatter
        formatted_question = format_question(seed_state, applied_rules, configuration)
        
        # Prepare the prompt for the LLM
        prompt = f"""
You are an expert at creating complex, multi-step questions for a question-answering system.
I'll provide you with a structured question template, and I want you to rewrite it into a 
natural, flowing question that sounds like it was written by a human.

The question should maintain all the logical steps and information requirements in the 
structured template, but make it read more naturally.

Structured question template:
{formatted_question}

Please rewrite this into a natural, flowing question. Make sure to include all necessary
information from each step, but phrase it in a way that reads as a cohesive whole rather
than separate numbered steps.
"""

        # Set up extra headers if site URL and name are provided
        extra_headers = {}
        if self.site_url:
            extra_headers["HTTP-Referer"] = self.site_url
        if self.site_name:
            extra_headers["X-Title"] = self.site_name

        # Generate the question using the LLM
        completion = self.client.chat.completions.create(
            extra_headers=extra_headers,
            model=self.model,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )

        # Return the generated question
        return completion.choices[0].message.content

    def generate(self, seed_state: State, **kwargs) -> Optional[str]:
        """
        Generate a question using the base generator for trajectory creation and the LLM for final formatting.

        Args:
            seed_state: The starting state for the question
            **kwargs: Additional arguments to pass to the base generator (e.g., max_hops, target_type)

        Returns:
            A natural language question generated by the LLM, or None if trajectory generation fails
        """
        # Use the base generator to create a trajectory
        print(f"Generating trajectory with {kwargs}")
        trajectory_result = self.base_generator.generate(seed_state, **kwargs)
        
        # Check if trajectory generation was successful
        if trajectory_result is None:
            print("Base generator failed to create a trajectory.")
            return None
            
        # Extract components from trajectory result
        # The base generators typically return (formatted_question, applied_rules, configuration)
        _, applied_rules, configuration = trajectory_result
        
        # Verify the right number of hops was generated
        if 'max_hops' in kwargs and len(applied_rules) != kwargs['max_hops']:
            print(f"Warning: Expected {kwargs['max_hops']} hops but got {len(applied_rules)}. Continuing anyway.")
            
        # Generate the question using the LLM
        return self.generate_from_trajectory(seed_state, applied_rules, configuration) 