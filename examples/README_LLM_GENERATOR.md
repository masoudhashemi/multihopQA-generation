# LLM Question Generator for MultihopQA

The LLM Question Generator takes the structured, step-by-step templates generated by the existing MultihopQA generators and transforms them into natural language questions that sound as if they were written by a human, while preserving all the logical steps and information requirements.

## Setup

### Prerequisites

1. You need an [OpenRouter](https://openrouter.ai/) account and API key
2. Python 3.6 or higher
3. The MultihopQA codebase
4. `python-dotenv` package installed (`pip install python-dotenv`)

### Environment Configuration

The LLM Question Generator uses environment variables to configure API keys and other settings. You can set these up in two ways:

#### Option 1: Using a .env file (Recommended)

Create a `.env` file in the root directory of the project by copying the provided `.env.example` file:

```bash
cp .env.example .env
```

Then edit the `.env` file to add your OpenRouter API key and other optional settings:

```
# OpenRouter API key (required)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Optional OpenRouter site information
SITE_URL=https://yoursite.com
SITE_NAME=Your Site Name

# Optional model configuration
LLM_MODEL=openai/gpt-4o
```

#### Option 2: Setting environment variables directly

You can also set environment variables directly in your terminal:

```bash
# For Linux/macOS
export OPENROUTER_API_KEY="your_api_key_here"

# For Windows (PowerShell)
$env:OPENROUTER_API_KEY="your_api_key_here"

# For Windows (Command Prompt)
set OPENROUTER_API_KEY=your_api_key_here
```

## Basic Usage

```python
from src.core import RULES_DB, InfoType, State
from src.generators import ForwardChainingGenerator, LLMQuestionGenerator

# Create a seed state
seed_state = State("Albert Einstein", InfoType.PERSON_NAME)

# Initialize a base generator
forward_generator = ForwardChainingGenerator(RULES_DB)

# Initialize the LLM Question Generator
# If you've set up the .env file or environment variables, no need to provide API key
llm_generator = LLMQuestionGenerator(base_generator=forward_generator)

# Generate a question
question = llm_generator.generate(seed_state, max_hops=3)
print(question)
```

## Using with Different Base Generators

The LLMQuestionGenerator works with any of the existing MultihopQA generators:

```python
# With ForwardChainingGenerator
llm_forward = LLMQuestionGenerator(ForwardChainingGenerator(RULES_DB))

# With GoalOrientedGenerator
llm_goal = LLMQuestionGenerator(GoalOrientedGenerator(RULES_DB))

# With BackwardChainingGenerator
llm_backward = LLMQuestionGenerator(BackwardChainingGenerator(RULES_DB))

# With TemplateBasedGenerator
llm_template = LLMQuestionGenerator(TemplateBasedGenerator(RULES_DB))

# With ConstrainedRandomWalkGenerator
llm_constrained = LLMQuestionGenerator(ConstrainedRandomWalkGenerator(RULES_DB))
```

## Working with Existing Trajectories

If you already have trajectory components (seed state, applied rules, and configuration), you can generate a question directly:

```python
# Assuming you have trajectory components from a previous run
seed_state = ...  # The initial State
applied_rules = ...  # List of Rule objects used
configuration = ...  # List of State objects (including seed_state)

# Initialize the LLM Question Generator
llm_generator = LLMQuestionGenerator(base_generator=None)  # API key from .env will be used

# Generate a question from the existing trajectory
question = llm_generator.generate_from_trajectory(seed_state, applied_rules, configuration)
print(question)
```

## Customizing the LLM Model

You can specify different OpenRouter-compatible models either in the .env file or when initializing the generator:

```python
# In .env file:
# LLM_MODEL=anthropic/claude-3-opus-20240229

# Or in code:
llm_generator = LLMQuestionGenerator(
    base_generator=forward_generator,
    model="anthropic/claude-3-opus-20240229"
)
```

See the [OpenRouter Models page](https://openrouter.ai/models) for a full list of available models.

## Example Script

See `examples/llm_question_example.py` for a complete working example of using the LLM Question Generator with environment variables. 